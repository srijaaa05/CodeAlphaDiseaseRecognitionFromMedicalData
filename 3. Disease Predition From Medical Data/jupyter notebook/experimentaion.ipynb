{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# for data preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# for imputing missing values\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "\n",
    "# import iterative imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# for machine learning\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,RandomizedSearchCV\n",
    "\n",
    "# Given that we anticipate solving our dependent feature through classification, we will proceed to import libraries tailored for classification tasks.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor ,AdaBoostClassifier ,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# for classification evaluation metrices\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_absolute_error,precision_score,mean_squared_error,r2_score\n",
    "\n",
    "# to perform statistical test\n",
    "from sklearn.feature_selection import chi2 # for categorical fetures\n",
    "from sklearn.feature_selection import f_classif # for numerical features (Anova f-test)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(r'E:\\CodeAlpha Machine Learing 1 month\\3. Disease Predition From Medical Data\\dataset\\heart_disease_uci.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    int64  \n",
      " 1   age       920 non-null    int64  \n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  861 non-null    float64\n",
      " 6   chol      890 non-null    float64\n",
      " 7   fbs       830 non-null    object \n",
      " 8   restecg   918 non-null    object \n",
      " 9   thalch    865 non-null    float64\n",
      " 10  exang     865 non-null    object \n",
      " 11  oldpeak   858 non-null    float64\n",
      " 12  slope     611 non-null    object \n",
      " 13  ca        309 non-null    float64\n",
      " 14  thal      434 non-null    object \n",
      " 15  num       920 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 115.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trestbps',\n",
       " 'chol',\n",
       " 'fbs',\n",
       " 'restecg',\n",
       " 'thalch',\n",
       " 'exang',\n",
       " 'oldpeak',\n",
       " 'slope',\n",
       " 'ca',\n",
       " 'thal']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.to_list()\n",
    "missing_data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"thal\",\"ca\",\"exang\",\"slope\",\"restecg\",\"fbs\",\"cp\",\"sex\",\"num\"]\n",
    "bool_cols = [\"fbs\",\"exang\"]\n",
    "numeric_cols = [\"oldpeak\",\"thalch\",\"chol\",\"trestbps\",\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to impute the missing values\n",
    "def impute_categorical_missing_data(passed_col):\n",
    "\n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "\n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    if passed_col in bool_cols:\n",
    "        y = label_encoder.fit_transform(y)\n",
    "\n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if len(df_null) > 0:\n",
    "        df_null[passed_col] = rf_classifier.predict(X)\n",
    "        if passed_col in bool_cols:\n",
    "            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "\n",
    "    return df_combined[passed_col]\n",
    "\n",
    "def impute_continuous_missing_data(passed_col):\n",
    "\n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "\n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_regressor = RandomForestRegressor()\n",
    "\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "    print(\"MAE =\", mean_absolute_error(y_test, y_pred), \"\\n\")\n",
    "    print(\"RMSE =\", mean_squared_error(y_test, y_pred, squared=False), \"\\n\")\n",
    "    print(\"R2 =\", r2_score(y_test, y_pred), \"\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if len(df_null) > 0:\n",
    "        df_null[passed_col] = rf_regressor.predict(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "\n",
    "    return df_combined[passed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values trestbps : 6.41%\n",
      "MAE = 12.966184971098263 \n",
      "\n",
      "RMSE = 16.970696658850077 \n",
      "\n",
      "R2 = 0.10075684900681159 \n",
      "\n",
      "Missing Values chol : 3.26%\n",
      "MAE = 45.22095505617977 \n",
      "\n",
      "RMSE = 64.17785128020273 \n",
      "\n",
      "R2 = 0.6737210992062744 \n",
      "\n",
      "Missing Values fbs : 9.78%\n",
      "The feature 'fbs' has been imputed with 79.52 accuracy\n",
      "\n",
      "Missing Values restecg : 0.22%\n",
      "The feature 'restecg' has been imputed with 66.3 accuracy\n",
      "\n",
      "Missing Values thalch : 5.98%\n",
      "MAE = 16.9206936416185 \n",
      "\n",
      "RMSE = 21.70553393649205 \n",
      "\n",
      "R2 = 0.31493739655281816 \n",
      "\n",
      "Missing Values exang : 5.98%\n",
      "The feature 'exang' has been imputed with 79.19 accuracy\n",
      "\n",
      "Missing Values oldpeak : 6.74%\n",
      "MAE = 0.5485581395348837 \n",
      "\n",
      "RMSE = 0.7730923784266633 \n",
      "\n",
      "R2 = 0.4305957172764855 \n",
      "\n",
      "Missing Values slope : 33.59%\n",
      "The feature 'slope' has been imputed with 66.67 accuracy\n",
      "\n",
      "Missing Values ca : 66.41%\n",
      "The feature 'ca' has been imputed with 64.52 accuracy\n",
      "\n",
      "Missing Values thal : 52.83%\n",
      "The feature 'thal' has been imputed with 68.97 accuracy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# impute missing values using our functions\n",
    "for col in missing_data_cols:\n",
    "    print(\"Missing Values\", col, \":\", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+\"%\")\n",
    "    if col in categorical_cols:\n",
    "        df[col] = impute_categorical_missing_data(col)\n",
    "    elif col in numeric_cols:\n",
    "        df[col] = impute_continuous_missing_data(col)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "age         0\n",
       "sex         0\n",
       "dataset     0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalch      0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'sex': ['Male' 'Female']\n",
      "Unique values in 'dataset': ['Cleveland' 'Hungary' 'Switzerland' 'VA Long Beach']\n",
      "Unique values in 'cp': ['typical angina' 'asymptomatic' 'non-anginal' 'atypical angina']\n",
      "Unique values in 'fbs': [True False]\n",
      "Unique values in 'restecg': ['lv hypertrophy' 'normal' 'st-t abnormality']\n",
      "Unique values in 'exang': [False True]\n",
      "Unique values in 'slope': ['downsloping' 'flat' 'upsloping']\n",
      "Unique values in 'thal': ['fixed defect' 'normal' 'reversable defect']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "# Calculate unique values for each categorical column\n",
    "unique_values = {col: df[col].unique() for col in categorical_cols}\n",
    "\n",
    "# Print the unique values\n",
    "for col, values in unique_values.items():\n",
    "    print(f\"Unique values in '{col}': {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the row from df where trestbps value is 0\n",
    "df[df['trestbps'] == 0]\n",
    "# remove this row from data\n",
    "df = df[df['trestbps'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical and numerical features\n",
    "categorical_columns = ['sex', 'cp', 'restecg', 'exang', 'slope', 'thal']\n",
    "numerical_columns = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']\n",
    "target_column = 'num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature set\n",
    "final_features = ['cp', 'exang', 'slope', 'thal', 'oldpeak', 'thalch', 'age', 'chol', 'trestbps']\n",
    "X = df[final_features]\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross-validation Accuracy: 0.5646258503401361\n",
      "Test Accuracy: 0.5706521739130435\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.5755102040816327\n",
      "Test Accuracy: 0.5978260869565217\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Cross-validation Accuracy: 0.5931972789115646\n",
      "Test Accuracy: 0.5543478260869565\n",
      "\n",
      "Model: Logistic Regression\n",
      "Cross-validation Accuracy: 0.5782312925170068\n",
      "Test Accuracy: 0.5706521739130435\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Cross-validation Accuracy: 0.5537414965986395\n",
      "Test Accuracy: 0.532608695652174\n",
      "\n",
      "Model: Decision Tree\n",
      "Cross-validation Accuracy: 0.48571428571428565\n",
      "Test Accuracy: 0.5054347826086957\n",
      "\n",
      "Model: Ada Boost\n",
      "Cross-validation Accuracy: 0.5333333333333333\n",
      "Test Accuracy: 0.5434782608695652\n",
      "\n",
      "Model: XG Boost\n",
      "Cross-validation Accuracy: 0.5687074829931973\n",
      "Test Accuracy: 0.5869565217391305\n",
      "\n",
      "Model: Naive Bayes\n",
      "Cross-validation Accuracy: 0.5700680272108843\n",
      "Test Accuracy: 0.5597826086956522\n",
      "\n",
      "Best Model: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model', GradientBoostingClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ada Boost', AdaBoostClassifier(random_state=42)),\n",
    "    ('XG Boost', XGBClassifier(random_state=42)),\n",
    "    ('Naive Bayes', GaussianNB())\n",
    "]\n",
    "\n",
    "# Hyperparameter tuning example for RandomForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate all models and select the best one\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(\"Model:\", name)\n",
    "    print(\"Cross-validation Accuracy:\", mean_accuracy)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "print(\"Best Model:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('02_heart_disease_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: No Heart Disease\n"
     ]
    }
   ],
   "source": [
    "# Sample data for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': [55],\n",
    "    'trestbps': [130],\n",
    "    'chol': [245],\n",
    "    'thalch': [205],\n",
    "    'oldpeak': [1.0],\n",
    "    'cp': [3],  # Assuming 1 corresponds to 'typical angina'\n",
    "    'exang': [1],\n",
    "    'slope': [3],  # Assuming 2 corresponds to 'downsloping'\n",
    "    'thal': [4]  # Assuming 2 corresponds to 'fixed defect'\n",
    "})\n",
    "\n",
    "# Ensure sample data has the same columns as the training data\n",
    "sample_data_encoded = sample_data[final_features]\n",
    "\n",
    "# Make predictions\n",
    "predicted = best_model.predict(sample_data_encoded)\n",
    "\n",
    "# Interpretation based on predicted value\n",
    "def interpret_prediction(prediction):\n",
    "    if prediction[0] == 0:\n",
    "        return \"No Heart Disease\"\n",
    "    elif prediction[0] == 1:\n",
    "        return \"Stage 1 (Mild Heart Disease)\"\n",
    "    elif prediction[0] == 2:\n",
    "        return \"Stage 2 (Moderate Heart Disease)\"\n",
    "    elif prediction[0] == 3:\n",
    "        return \"Stage 3 (Advanced Heart Disease)\"\n",
    "    elif prediction[0] == 4:\n",
    "        return \"Stage 4 (Severe Heart Disease)\"\n",
    "    else:\n",
    "        return \"Unknown Stage\"\n",
    "\n",
    "# Print the result\n",
    "print(\"Prediction:\", interpret_prediction(predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Stage 2 (Moderate Heart Disease)\n"
     ]
    }
   ],
   "source": [
    "# Sample data for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': [67],\n",
    "    'trestbps': [160],\n",
    "    'chol': [286],\n",
    "    'thalch': [108],\n",
    "    'oldpeak': [1.5],\n",
    "    'cp': [1],  # Assuming 1 corresponds to 'typical angina'\n",
    "    'exang': [1],\n",
    "    'slope': [1],  # Assuming 2 corresponds to 'downsloping'\n",
    "    'thal': [1]  # Assuming 2 corresponds to 'fixed defect'\n",
    "})\n",
    "\n",
    "# Ensure sample data has the same columns as the training data\n",
    "sample_data_encoded = sample_data[final_features]\n",
    "\n",
    "# Make predictions\n",
    "predicted = best_model.predict(sample_data_encoded)\n",
    "\n",
    "# Interpretation based on predicted value\n",
    "def interpret_prediction(prediction):\n",
    "    if prediction[0] == 0:\n",
    "        return \"No Heart Disease\"\n",
    "    elif prediction[0] == 1:\n",
    "        return \"Stage 1 (Mild Heart Disease)\"\n",
    "    elif prediction[0] == 2:\n",
    "        return \"Stage 2 (Moderate Heart Disease)\"\n",
    "    elif prediction[0] == 3:\n",
    "        return \"Stage 3 (Advanced Heart Disease)\"\n",
    "    elif prediction[0] == 4:\n",
    "        return \"Stage 4 (Severe Heart Disease)\"\n",
    "    else:\n",
    "        return \"Unknown Stage\"\n",
    "\n",
    "# Print the result\n",
    "print(\"Prediction:\", interpret_prediction(predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
